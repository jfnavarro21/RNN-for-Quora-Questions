{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "f765eb10-54e6-4a14-912b-e5117e8f433d"
    }
   },
   "source": [
    "# Advanced Machine Learning (MScA, 32017)\n",
    "\n",
    "# Project: Paraphrase Detection\n",
    "\n",
    "# Part 2: Solution by recurrent neural network\n",
    "\n",
    "### Yuri Balasanov, Leonid Nazarov, &copy; iLykei 2017\n",
    "\n",
    "Keras provides tools for Natural Language Processing including preprocessing text and working with pre-trained word embeddings. The goal of this notebook is to describe those tools and to give some basic examples of using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import dependencies \n",
    "#%matplotlib inline\n",
    "import numpy as np\n",
    "from keras.layers import Dense, Activation, LSTM, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adagrad, adam\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing.text import text_to_word_sequence, one_hot, Tokenizer \n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely? How can I solve...   \n",
       "4  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  \n",
       "0  What is the step by step guide to invest in sh...  \n",
       "1  What would happen if the Indian government sto...  \n",
       "2  How can Internet speed be increased by hacking...  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...  \n",
       "4            Which fish would survive in salt water?  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the train data (1000 rows)\n",
    "dataPath = \"./data/\"\n",
    "#train = pd.read_csv(dataPath+'quora_train_1000.csv',usecols=['question1','question2'])\n",
    "train = pd.read_csv(dataPath+'train_sample.csv',usecols=['question1','question2'])\n",
    "train.dropna(inplace=True) # remove two rows as in NLP feature creation\n",
    "train = train[:1000] # only for demo and testing, comment out with complete data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps of Preprocessing\n",
    "\n",
    "Description of all Keras tools necessary for converting questions into additional useful features for neural network is over. \n",
    "\n",
    "Now go through the several following steps of processing questions. \n",
    "\n",
    "## Step 1. Lemmatization\n",
    "\n",
    "Questions are preprocessed so that the different forms of writing the same text (like \"don't\" and \"do not\") are  matched. Lemmatization similar to one done in the first part of the project helps again. \n",
    "\n",
    "Lemmatize with *WordNetLemmatizer*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create cutter function\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "WNL = WordNetLemmatizer()\n",
    "\n",
    "def cutter(word):\n",
    "    if len(word) < 4:\n",
    "        return word\n",
    "    return WNL.lemmatize(WNL.lemmatize(word, \"n\"), \"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create preprocess function (uses cutter)\n",
    "\n",
    "def preprocess(string):\n",
    "    # standardize expression with apostrophe, replace some special symbols with word\n",
    "    string = string.lower().replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\") \\\n",
    "        .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\") \\\n",
    "        .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\") \\\n",
    "        .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\") \\\n",
    "        .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\") \\\n",
    "        .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \") \\\n",
    "        .replace(\"€\", \" euro \").replace(\"'ll\", \" will\").replace(\"=\", \" equal \").replace(\"+\", \" plus \")\n",
    "    # remove punctuation and special symbols\n",
    "    string = re.sub('[“”\\(\\'…\\)\\!\\^\\\"\\.;:,\\-\\?？\\{\\}\\[\\]\\\\/\\*@]', ' ', string)\n",
    "    string = re.sub(r\"([0-9]+)000000\", r\"\\1m\", string)\n",
    "    string = re.sub(r\"([0-9]+)000\", r\"\\1k\", string)\n",
    "    # lemmatize\n",
    "    string = ' '.join([cutter(w) for w in string.split()])\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply preprocessing to train sample. \n",
    "\n",
    "All transformations applied to train should be applied to test too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: What is the story of Kohinoor (Koh-i-Noor) Diamond?\n",
      "Question 2: What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?\n",
      "Question 1 processed: what is the story of kohinoor koh i noor diamond\n",
      "Question 2 processed: what would happen if the indian government steal the kohinoor koh i noor diamond back\n"
     ]
    }
   ],
   "source": [
    "# run preprocess function on all of train set\n",
    "\n",
    "print('Question 1: %s' % train[\"question1\"][1])\n",
    "print('Question 2: %s' % train[\"question2\"][1])\n",
    "train[\"question1\"] = train[\"question1\"].fillna(\"\").apply(preprocess)\n",
    "train[\"question2\"] = train[\"question2\"].fillna(\"\").apply(preprocess)\n",
    "print('Question 1 processed: %s' % train.question1[1])\n",
    "print('Question 2 processed: %s' % train.question2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Creating vocabulary of frequent words\n",
    "\n",
    "Create vocabulary of relatively frequent words in questions: words with frequency greater than *MIN_WORD_OCCURRENCE* times. \n",
    "\n",
    "For the small dataset *MIN_WORD_OCCURRENCE* is selected small, but for the whole dataset it should be much larger (may be in the range 50-150).\n",
    "\n",
    "For word count use familiar *CountVectorizer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 top_words\n",
      "Top words ['be', 'of', 'on', 'it', 'why', 'what', 'best', 'do', 'how', 'can']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "MIN_WORD_OCCURRENCE = 100 # 3 for demo and testing in local environment. Select number for final results\n",
    "\n",
    "all_questions = pd.Series(train[\"question1\"].tolist() + train[\"question2\"].tolist()).unique()\n",
    "vectorizer = CountVectorizer(lowercase=False, token_pattern=\"\\S+\", # replace white spaces with spaces\n",
    "                             min_df=MIN_WORD_OCCURRENCE)\n",
    "vectorizer.fit(all_questions)\n",
    "top_words = set(vectorizer.vocabulary_.keys())\n",
    "print(len(top_words),'top_words')\n",
    "print('Top words %s' % list(top_words)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Remove rare words\n",
    "\n",
    "The consecutive rare words are replaced with one word \"suspense\" (you may try another replacement). The result is limited to 30 trailing words. \n",
    "\n",
    "Remove first words in long question since the end of it is usually more important. \n",
    "\n",
    "Add \"suspense\" to *top_words*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "REPLACE_WORD = \"suspense\"\n",
    "top_words.add(REPLACE_WORD)\n",
    "MAX_SEQUENCE_LENGTH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: motorola company can i hack my charter motorolla dcx3400\n",
      "Prepared question: suspense can i suspense my suspense\n"
     ]
    }
   ],
   "source": [
    "def prepare(q):\n",
    "    new_q = []\n",
    "    new_suspense = True # ready to add REPLACE_WORD \n",
    "    # a[::-1] invert order of list a, so we start from the end\n",
    "    for w in q.split()[::-1]:\n",
    "        if w in top_words:\n",
    "            new_q = [w] + new_q # add word from top_words\n",
    "            new_suspense = True\n",
    "        elif new_suspense:\n",
    "            new_q = [REPLACE_WORD] + new_q\n",
    "            new_suspense = False  # only 1 REPLACE_WORD for group of rare words\n",
    "        if len(new_q) == MAX_SEQUENCE_LENGTH:\n",
    "            break\n",
    "    new_q = \" \".join(new_q)\n",
    "    return new_q\n",
    "\n",
    "question = train.question1[9]\n",
    "print('Question: %s' % question)\n",
    "print('Prepared question: %s' % prepare(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the function to train questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the suspense to suspense in suspense in suspense\n"
     ]
    }
   ],
   "source": [
    "q1s_train = train.question1.apply(prepare)\n",
    "q2s_train = train.question2.apply(prepare)\n",
    "print(q1s_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Create embedding index\n",
    "\n",
    "Build embedding index - dictionary with words from *top_words* as keys and their vector presentations as values.\n",
    "\n",
    "Take vector presentations of words from Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download) embedding file [glove.840B.300d](http://nlp.stanford.edu/data/glove.840B.300d.zip). Each line of the file contains word space separated from components of word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "EMBEDDING_FILE = \"./glove.840B.300d.txt\"\n",
    "\n",
    "def get_embedding():\n",
    "    embeddings_index = {}\n",
    "    with open(EMBEDDING_FILE, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            if len(values) == EMBEDDING_DIM + 1 and word in top_words:\n",
    "                coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "                embeddings_index[word] = coefs\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build *embeddings_index* and reduce *top_words* to those having vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words not found in the embedding: set()\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = get_embedding()\n",
    "print(\"Words not found in the embedding:\", top_words - embeddings_index.keys())\n",
    "top_words = embeddings_index.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Transform questions into integer valued sequences of equal lengths\n",
    "\n",
    "It is described above how *Tokenizer.texts_to_sequences* converts question to a list of integers. \n",
    "\n",
    "But such lists may have different lengths for different questions. \n",
    "\n",
    "Keras provides method for fixing this issue:\n",
    "\n",
    "*keras.preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.)* \n",
    "\n",
    "It transforms a list of *num_samples* sequences (lists of scalars) into a 2D Numpy array of shape \n",
    "\n",
    "*(num_samples, num_timesteps)*, \n",
    "\n",
    "where *num_timesteps* is either *maxlen* argument (if provided), or the length of the longest sequence.\n",
    "\n",
    "Sequences that are shorter than *num_timesteps* are padded with *value* at the end. Sequences longer than *num_timesteps* are truncated so that they have the desired length. \n",
    "\n",
    "Position where padding or truncation happens is determined by *padding* or *truncating*, respectively.\n",
    "\n",
    "Here are several examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sequences: [[1, 2], [1, 2, 3, 4, 5]]\n",
      "Padded default: [[0 0 0 1 2]\n",
      " [1 2 3 4 5]]\n",
      "Padded with maxlen=4: [[0 0 1 2]\n",
      " [2 3 4 5]]\n",
      "Padded with maxlen=4, padding=post: [[1 2 0 0]\n",
      " [2 3 4 5]]\n",
      "Padded with maxlen=4, padding=post, truncating=post: [[1 2 0 0]\n",
      " [1 2 3 4]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "sequences = [[1,2],[1,2,3,4,5]]\n",
    "print('Original sequences: %s' % sequences)\n",
    "print('Padded default: %s' % pad_sequences(sequences))\n",
    "print('Padded with maxlen=4: %s' % pad_sequences(sequences,maxlen=4))\n",
    "print('Padded with maxlen=4, padding=post: %s' % pad_sequences(sequences,maxlen=4,padding='post'))\n",
    "print('Padded with maxlen=4, padding=post, truncating=post: %s' \\\n",
    "      %pad_sequences(sequences,maxlen=4,padding='post',truncating='post'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit *Tokenizer* to the questions obtained after Step 3 and apply *texts_to_sequences* and *pad_sequences* to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final representation of first question 1:\n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  4  2  1\n",
      "   7  1  9  1  9  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  4\n",
      "   2  1 11  1  5  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  6 14  5  1  2\n",
      "   1 11 18  1  8  1]]\n",
      "Final representation of first question 2:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 4 2 1 7 1 9 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(filters=\"\")\n",
    "tokenizer.fit_on_texts(np.append(q1s_train, q2s_train))\n",
    "word_index = tokenizer.word_index\n",
    "data_1 = pad_sequences(tokenizer.texts_to_sequences(q1s_train), \n",
    "                       maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(tokenizer.texts_to_sequences(q2s_train), \n",
    "                       maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Final representation of first question 1:')\n",
    "print(data_1[:3])\n",
    "print('Final representation of first question 2:')\n",
    "print(data_2[0])\n",
    "len(data_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each question now is represented by a vector of 30 numbers.\n",
    "\n",
    "Repeat the same steps with *test* set and create:\n",
    "\n",
    "*q1s_test -> test_data_1*  \n",
    "*q2s_test -> test_data_2*  \n",
    "\n",
    "Do not refit Tokenizer, use the same as for *train*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(dataPath+'test_sample.csv',usecols=['question1','question2'])\n",
    "test[\"question1\"] = test[\"question1\"].fillna(\"\").apply(preprocess)\n",
    "test[\"question2\"] = test[\"question2\"].fillna(\"\").apply(preprocess)\n",
    "q1s_test = test.question1.apply(prepare)\n",
    "q2s_test = test.question2.apply(prepare)\n",
    "test_data_1 = pad_sequences(tokenizer.texts_to_sequences(q1s_test), \n",
    "                       maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data_2 = pad_sequences(tokenizer.texts_to_sequences(q2s_test), \n",
    "                       maxlen=MAX_SEQUENCE_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data_1.shape\n",
    "#test_data_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Create embedding matrix\n",
    "\n",
    "Now make embedding matrix of weights from embedding index. \n",
    "\n",
    "The *i-th* row of this matrix is a vector representation of word with index *i* in *word_index*. \n",
    "\n",
    "The embedding matrix will be used as weights matrix for embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))  # matrix of zeros\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create embedding layer from embedding matrix as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(nb_words, EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting *trainable=False* declares that no changing weights is required during traning. \n",
    "\n",
    "This layer just transforms sequences of integers (word indexes) into sequences of their vector representations.  \n",
    "\n",
    "## Step 7. Save the data\n",
    "\n",
    "We prepared the the following variables for neural network:\n",
    "\n",
    "\n",
    "- *data_1*, *data_2*: padded numeric sequences for questions 1 and 2 in train sample \n",
    "- *test_data_1*, *test_data_2*: padded numeric sequences for questions 1 and 2 in test sample\n",
    "- *nb_words*: length of dictionary *'word_index'* \n",
    "- *embedding_matrix*: matrix for transformation in the embedding layer\n",
    "\n",
    "Save these variables to *.pkl* files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-76ad76eed4cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./savedData/nb_words.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./savedData/embedding_matrix.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./savedData/test_data_1.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./savedData/test_data_2.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_data_1' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('./savedData/data_1.pkl', 'wb') as f: pickle.dump(data_1, f, -1)\n",
    "with open('./savedData/data_2.pkl', 'wb') as f: pickle.dump(data_2, f, -1)\n",
    "with open('./savedData/nb_words.pkl', 'wb') as f: pickle.dump(nb_words, f, -1)\n",
    "with open('./savedData/embedding_matrix.pkl', 'wb') as f: pickle.dump(embedding_matrix, f, -1)\n",
    "with open('./savedData/test_data_1.pkl', 'wb') as f: pickle.dump(test_data_1, f, -1)\n",
    "with open('./savedData/test_data_2.pkl', 'wb') as f: pickle.dump(test_data_2, f, -1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network will also use NLP features obtained using Spark in the first part of the project.\n",
    "\n",
    "# Nework architecture\n",
    "\n",
    "Quora released a [public dataset of duplicate questions](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs) before the  competition, so, some interesting solutions had been already available before it started. \n",
    "\n",
    "Among them were approaches from:\n",
    "\n",
    "- [Quora hackathon](https://engineering.quora.com/Semantic-Question-Matching-with-Deep-Learning), \n",
    "- [Deep learning model](https://github.com/bradleypallen/keras-quora-question-pairs)  by Bradley Pallen et al.\n",
    "\n",
    "Those approaches extensively use Recurrent Neural Networks usually with Long Short-Term Memory (LSTM) layers. \n",
    "\n",
    "The competition also showed the power of these methods. <br>\n",
    "\n",
    "Participant [aphex34](https://www.kaggle.com/aphex34) was the only solo competitor among top 10 teams. He used NN techniques on all stages of reasearch including feature engeneering (see [7-th solution overview](https://www.kaggle.com/c/quora-question-pairs/discussion/34697#192676)). But his code has not been published.  \n",
    "\n",
    "In order to solve the problem it is recommended to implement Ahmet Erdem's architecture which is relatively simple and also uses LSTM network.\n",
    "![Ahmed LSTM](https://ilykei.com/api/fileProxy/documents%2FAdvanced%20Machine%20Learning%2FLecture%207%20AdvML%2FAhmedAlgo.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network has 3 inputs: \n",
    "\n",
    "- input_1 and input_2 for questions converted to matrices (*data_1, data_2*) \n",
    "- and input_3 for NLP features. \n",
    "\n",
    "Questions share the same embedding_1 and lstm_1 layers. \n",
    "\n",
    "Denote *y1* and *y2* outputs of layer *'lstm_1'* corresponding to the first and the second inputs, respectively.\n",
    "\n",
    "Calculation inside red square is vector of squared differences of 2 outputs of layer *'lstm_1'*:\n",
    "\n",
    "1. Output *y1* is miltiplied by -1 in lambda_1 layer \n",
    "2. Then the result is added to *y2* in layer *'add_2'*. So, the output of layer *'add_2'* is difference between *y1* and *y2*. (Alternatively you can apply subtraction shown in Keras_basics.ipynb). \n",
    "3. The vector of differences is multiplied by itself element-wise in *'multiply_1'* layer. The result is vector of squared differences.\n",
    "\n",
    "Then the vector of squared differences is concatenated in layer *'concatenate_1'* with sum of *y1* and *y2* obtained in layer *'add_1'*.  \n",
    "\n",
    "The loss function to be minimized is *loss='binary_crossentropy'*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task description\n",
    "\n",
    "1. Prepare train and test data for network in local environment.\n",
    "2. Implement the network above and tune it in local environment with part of the train data. <br>\n",
    "    Parameters to be tuned are: number of neurons in each layer, dropout rates (including recurrent_dropout of LSTM layer), standard deviation of *GaussianNoise* layer, *batch_size*. \n",
    "3. Run the model on the cluster with complete data and generate submission file as follows:\n",
    "\n",
    "*submission = pd.DataFrame({\"test_id\": test_id, \"is_duplicate\": prediction_prob})*  \n",
    "*submission.to_csv(\"submission1.csv\", index=False)*,\n",
    "\n",
    "where *prediction_prob* is 1D array of prediction probabilities, *test_id* is index from *test_id* column of *test.csv* file.\n",
    "\n",
    "Example of sbatch file to run task on GPU is given below (do not forget to remove end of line symbols < br > at the end of each line):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-c97283b98e65>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0membedding_matrix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'savedData/test_data_1.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mtest_data_1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'savedData/test_data_2.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mtest_data_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "# load pickle files\n",
    "with open('savedData/data_1.pkl', 'rb') as f:\n",
    "    data_1 = pickle.load(f)\n",
    "with open('savedData/data_2.pkl', 'rb') as f:\n",
    "    data_2 = pickle.load(f)\n",
    "with open('savedData/nb_words.pkl', 'rb') as f: \n",
    "    nb_words = pickle.load(f)\n",
    "with open('savedData/embedding_matrix.pkl', 'rb') as f: \n",
    "    embedding_matrix=pickle.load(f)\n",
    "with open('savedData/test_data_1.pkl', 'rb') as f: \n",
    "    test_data_1=pickle.load(f)\n",
    "with open('savedData/test_data_2.pkl', 'rb') as f: \n",
    "    test_data_2 = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 22)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load features matrix from week 5\n",
    "data_3 = np.loadtxt('train_features_1000.csv', delimiter=\",\", skiprows=1)\n",
    "data_3.shape #1000x23\n",
    "\n",
    "#data_1.shape # 1000x30\n",
    "Ytrain=data_3[:,-1]\n",
    "\n",
    "\n",
    "# remove last column of data_3\n",
    "data_3=data_3[:,0:22]\n",
    "data_3.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ytrain[1:10,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "MAX_SEQUENCE_LENGTH=30\n",
    "from keras.layers import Input, LSTM, BatchNormalization, GaussianNoise, \\\n",
    "concatenate, Lambda, add, Embedding, Dropout#,subtract\n",
    "def getModel(lstmneurons=10, droprate=.1,  neurons1=10, neurons2=10, stdev=.1):\n",
    "    # Three input layers\n",
    "    input1 = Input(shape=(30,), name='input1')  # data_1: padded vector of 30 numbers\n",
    "    input2 = Input(shape=(30,), name='input2') # data_2: padded vector of 30 numbers\n",
    "    input3 = Input(shape=(22,), name='input3') # train_features_1000\n",
    "    # embeddings layer\n",
    "    embedding1 = Embedding(nb_words, EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)(input1)\n",
    "    embedding2 = Embedding(nb_words, EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=False)(input2)\n",
    "    # lstm layer\n",
    "    lstm1 = LSTM(lstmneurons, activation='tanh', recurrent_activation='hard_sigmoid')(embedding1)\n",
    "    # lstm layer\n",
    "    lstm2 = LSTM(lstmneurons, activation='tanh', recurrent_activation='hard_sigmoid')(embedding2)\n",
    "    # double the values\n",
    "    add1 = add([lstm1, lstm2])\n",
    "    # Create squared differences\n",
    "    lambda1 = Lambda(lambda x: -1 * x)(lstm1)\n",
    "    add2 = add([lstm2, lambda1])\n",
    "    #add2=subtract([lstm1,lambda1])\n",
    "    multiply1 = Lambda(lambda x: x**2)(add2)\n",
    "    # combining squared differences and added values\n",
    "    concatenate1 = concatenate([multiply1,add1], name='concatenate1') \n",
    "    # normalize the activations\n",
    "    batch_normalization1 = BatchNormalization()(input3)\n",
    "    # dense layer\n",
    "    dense1 = Dense(neurons1, activation='relu', name='dense1')(batch_normalization1)\n",
    "    # dropout - ignore portion of neuorns (regularization technique)\n",
    "    dropout1 = Dropout(droprate, name='dropout1')(dense1)\n",
    "    dropout2 = Dropout(droprate, name='dropout2')(concatenate1)\n",
    "    # combine the two dropout layers\n",
    "    concatenate2 = concatenate([dropout1,dropout2], name='concatenate2')\n",
    "    # Normalize the activations\n",
    "    batch_normalization2 = BatchNormalization()(concatenate2)\n",
    "    # adding noise, zero centered\n",
    "    gaussian_noise1 = GaussianNoise(stdev)(batch_normalization2)\n",
    "    # dense layer\n",
    "    dense2 = Dense(neurons2, activation='relu', name='dense2')(gaussian_noise1)\n",
    "    # 3rd dropout layer\n",
    "    dropout3 = Dropout(droprate, name='dropout3')(dense2)\n",
    "    # Use softmax for output layer\n",
    "    output1 = Dense(1, activation='sigmoid', name='output1')(dropout3)\n",
    "    # define inputs and outputs for model\n",
    "    model = Model(inputs=[input1,input2,input3], outputs=output1)\n",
    "    # configure the model for training\n",
    "    model.compile(optimizer='Adagrad', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/200\n",
      "1s - loss: 0.7481 - acc: 0.5712 - val_loss: 1.2736 - val_acc: 0.6000\n",
      "Epoch 2/200\n",
      "0s - loss: 0.6709 - acc: 0.6188 - val_loss: 0.9150 - val_acc: 0.6000\n",
      "Epoch 3/200\n",
      "0s - loss: 0.6383 - acc: 0.6525 - val_loss: 0.7929 - val_acc: 0.6000\n",
      "Epoch 4/200\n",
      "0s - loss: 0.6058 - acc: 0.6650 - val_loss: 0.7209 - val_acc: 0.6050\n",
      "Epoch 5/200\n",
      "0s - loss: 0.5991 - acc: 0.6512 - val_loss: 0.6737 - val_acc: 0.6200\n",
      "Epoch 6/200\n",
      "0s - loss: 0.5776 - acc: 0.6900 - val_loss: 0.6356 - val_acc: 0.6400\n",
      "Epoch 7/200\n",
      "0s - loss: 0.5597 - acc: 0.6938 - val_loss: 0.6115 - val_acc: 0.6700\n",
      "Epoch 8/200\n",
      "0s - loss: 0.5621 - acc: 0.6825 - val_loss: 0.6029 - val_acc: 0.6700\n",
      "Epoch 9/200\n",
      "0s - loss: 0.5648 - acc: 0.6975 - val_loss: 0.5953 - val_acc: 0.6600\n",
      "Epoch 10/200\n",
      "0s - loss: 0.5547 - acc: 0.6987 - val_loss: 0.5934 - val_acc: 0.6800\n",
      "Epoch 11/200\n",
      "0s - loss: 0.5518 - acc: 0.7187 - val_loss: 0.5754 - val_acc: 0.7050\n",
      "Epoch 12/200\n",
      "0s - loss: 0.5332 - acc: 0.7300 - val_loss: 0.5702 - val_acc: 0.7000\n",
      "Epoch 13/200\n",
      "0s - loss: 0.5251 - acc: 0.7250 - val_loss: 0.5675 - val_acc: 0.6950\n",
      "Epoch 14/200\n",
      "0s - loss: 0.5248 - acc: 0.7138 - val_loss: 0.5631 - val_acc: 0.6950\n",
      "Epoch 15/200\n",
      "0s - loss: 0.5288 - acc: 0.7300 - val_loss: 0.5578 - val_acc: 0.7050\n",
      "Epoch 16/200\n",
      "0s - loss: 0.5208 - acc: 0.7200 - val_loss: 0.5522 - val_acc: 0.7100\n",
      "Epoch 17/200\n",
      "0s - loss: 0.5163 - acc: 0.7313 - val_loss: 0.5515 - val_acc: 0.7150\n",
      "Epoch 18/200\n",
      "0s - loss: 0.5147 - acc: 0.7400 - val_loss: 0.5495 - val_acc: 0.7250\n",
      "Epoch 19/200\n",
      "0s - loss: 0.5111 - acc: 0.7150 - val_loss: 0.5467 - val_acc: 0.7250\n",
      "Epoch 20/200\n",
      "0s - loss: 0.5089 - acc: 0.7313 - val_loss: 0.5439 - val_acc: 0.7150\n",
      "Epoch 21/200\n",
      "0s - loss: 0.4928 - acc: 0.7562 - val_loss: 0.5432 - val_acc: 0.7250\n",
      "Epoch 22/200\n",
      "0s - loss: 0.5031 - acc: 0.7425 - val_loss: 0.5394 - val_acc: 0.7150\n",
      "Epoch 23/200\n",
      "0s - loss: 0.4952 - acc: 0.7500 - val_loss: 0.5375 - val_acc: 0.7050\n",
      "Epoch 24/200\n",
      "0s - loss: 0.4896 - acc: 0.7525 - val_loss: 0.5359 - val_acc: 0.7050\n",
      "Epoch 25/200\n",
      "0s - loss: 0.4888 - acc: 0.7338 - val_loss: 0.5355 - val_acc: 0.7150\n",
      "Epoch 26/200\n",
      "0s - loss: 0.4813 - acc: 0.7575 - val_loss: 0.5351 - val_acc: 0.7150\n",
      "Epoch 27/200\n",
      "0s - loss: 0.4814 - acc: 0.7450 - val_loss: 0.5312 - val_acc: 0.7100\n",
      "Epoch 28/200\n",
      "0s - loss: 0.4720 - acc: 0.7525 - val_loss: 0.5313 - val_acc: 0.7050\n",
      "Epoch 29/200\n",
      "0s - loss: 0.4893 - acc: 0.7437 - val_loss: 0.5332 - val_acc: 0.7000\n",
      "Epoch 30/200\n",
      "0s - loss: 0.4643 - acc: 0.7537 - val_loss: 0.5299 - val_acc: 0.7200\n",
      "Epoch 31/200\n",
      "0s - loss: 0.4637 - acc: 0.7587 - val_loss: 0.5255 - val_acc: 0.7300\n",
      "Epoch 32/200\n",
      "0s - loss: 0.4754 - acc: 0.7650 - val_loss: 0.5257 - val_acc: 0.7250\n",
      "Epoch 33/200\n",
      "0s - loss: 0.4530 - acc: 0.7750 - val_loss: 0.5252 - val_acc: 0.7050\n",
      "Epoch 34/200\n",
      "0s - loss: 0.4561 - acc: 0.7662 - val_loss: 0.5236 - val_acc: 0.7150\n",
      "Epoch 35/200\n",
      "0s - loss: 0.4575 - acc: 0.7750 - val_loss: 0.5227 - val_acc: 0.7150\n",
      "Epoch 36/200\n",
      "0s - loss: 0.4460 - acc: 0.7700 - val_loss: 0.5204 - val_acc: 0.7200\n",
      "Epoch 37/200\n",
      "0s - loss: 0.4485 - acc: 0.7737 - val_loss: 0.5201 - val_acc: 0.7100\n",
      "Epoch 38/200\n",
      "0s - loss: 0.4554 - acc: 0.7712 - val_loss: 0.5226 - val_acc: 0.6950\n",
      "Epoch 39/200\n",
      "0s - loss: 0.4467 - acc: 0.7650 - val_loss: 0.5211 - val_acc: 0.7100\n",
      "Epoch 40/200\n",
      "0s - loss: 0.4492 - acc: 0.7650 - val_loss: 0.5176 - val_acc: 0.7100\n",
      "Epoch 41/200\n",
      "0s - loss: 0.4439 - acc: 0.7750 - val_loss: 0.5169 - val_acc: 0.7050\n",
      "Epoch 42/200\n",
      "0s - loss: 0.4404 - acc: 0.7725 - val_loss: 0.5179 - val_acc: 0.7050\n",
      "Epoch 43/200\n",
      "0s - loss: 0.4432 - acc: 0.7738 - val_loss: 0.5153 - val_acc: 0.7050\n",
      "Epoch 44/200\n",
      "0s - loss: 0.4352 - acc: 0.7825 - val_loss: 0.5186 - val_acc: 0.7000\n",
      "Epoch 45/200\n",
      "0s - loss: 0.4275 - acc: 0.7900 - val_loss: 0.5154 - val_acc: 0.7150\n",
      "Epoch 46/200\n",
      "0s - loss: 0.4318 - acc: 0.7813 - val_loss: 0.5175 - val_acc: 0.7200\n",
      "Epoch 47/200\n",
      "0s - loss: 0.4271 - acc: 0.7900 - val_loss: 0.5190 - val_acc: 0.6950\n",
      "Epoch 48/200\n",
      "0s - loss: 0.4237 - acc: 0.7863 - val_loss: 0.5182 - val_acc: 0.6900\n",
      "Epoch 49/200\n",
      "0s - loss: 0.4209 - acc: 0.7838 - val_loss: 0.5185 - val_acc: 0.7050\n",
      "Epoch 50/200\n",
      "0s - loss: 0.4168 - acc: 0.8000 - val_loss: 0.5123 - val_acc: 0.7050\n",
      "Epoch 51/200\n",
      "0s - loss: 0.4333 - acc: 0.7862 - val_loss: 0.5116 - val_acc: 0.7150\n",
      "Epoch 52/200\n",
      "0s - loss: 0.4168 - acc: 0.7950 - val_loss: 0.5117 - val_acc: 0.7100\n",
      "Epoch 53/200\n",
      "0s - loss: 0.4058 - acc: 0.7912 - val_loss: 0.5100 - val_acc: 0.7150\n",
      "Epoch 54/200\n",
      "0s - loss: 0.4012 - acc: 0.8075 - val_loss: 0.5089 - val_acc: 0.7200\n",
      "Epoch 55/200\n",
      "0s - loss: 0.4079 - acc: 0.8000 - val_loss: 0.5109 - val_acc: 0.7150\n",
      "Epoch 56/200\n",
      "0s - loss: 0.4048 - acc: 0.8050 - val_loss: 0.5146 - val_acc: 0.7150\n",
      "Epoch 57/200\n",
      "0s - loss: 0.4084 - acc: 0.7888 - val_loss: 0.5120 - val_acc: 0.7400\n",
      "Epoch 58/200\n",
      "0s - loss: 0.3967 - acc: 0.8100 - val_loss: 0.5123 - val_acc: 0.7300\n",
      "Epoch 59/200\n",
      "0s - loss: 0.4079 - acc: 0.7950 - val_loss: 0.5108 - val_acc: 0.7200\n",
      "Epoch 60/200\n",
      "0s - loss: 0.3901 - acc: 0.8125 - val_loss: 0.5078 - val_acc: 0.7150\n",
      "Epoch 61/200\n",
      "0s - loss: 0.3896 - acc: 0.8250 - val_loss: 0.5076 - val_acc: 0.7300\n",
      "Epoch 62/200\n",
      "0s - loss: 0.3978 - acc: 0.8100 - val_loss: 0.5142 - val_acc: 0.7250\n",
      "Epoch 63/200\n",
      "0s - loss: 0.3910 - acc: 0.8100 - val_loss: 0.5101 - val_acc: 0.7300\n",
      "Epoch 64/200\n",
      "0s - loss: 0.3876 - acc: 0.8225 - val_loss: 0.5068 - val_acc: 0.7500\n",
      "Epoch 65/200\n",
      "0s - loss: 0.3823 - acc: 0.8238 - val_loss: 0.5107 - val_acc: 0.7300\n",
      "Epoch 66/200\n",
      "0s - loss: 0.3875 - acc: 0.8125 - val_loss: 0.5118 - val_acc: 0.7200\n",
      "Epoch 67/200\n",
      "0s - loss: 0.3664 - acc: 0.8387 - val_loss: 0.5120 - val_acc: 0.7250\n",
      "Epoch 68/200\n",
      "0s - loss: 0.3769 - acc: 0.8288 - val_loss: 0.5078 - val_acc: 0.7250\n",
      "Epoch 69/200\n",
      "0s - loss: 0.3817 - acc: 0.8125 - val_loss: 0.5103 - val_acc: 0.7300\n",
      "Epoch 70/200\n",
      "0s - loss: 0.3701 - acc: 0.8237 - val_loss: 0.5098 - val_acc: 0.7200\n",
      "Epoch 71/200\n",
      "0s - loss: 0.3730 - acc: 0.8237 - val_loss: 0.5106 - val_acc: 0.7350\n",
      "Epoch 72/200\n",
      "0s - loss: 0.3609 - acc: 0.8338 - val_loss: 0.5092 - val_acc: 0.7300\n",
      "Epoch 73/200\n",
      "0s - loss: 0.3552 - acc: 0.8450 - val_loss: 0.5089 - val_acc: 0.7200\n",
      "Epoch 74/200\n",
      "0s - loss: 0.3622 - acc: 0.8362 - val_loss: 0.5126 - val_acc: 0.7200\n",
      "Epoch 75/200\n",
      "0s - loss: 0.3640 - acc: 0.8312 - val_loss: 0.5118 - val_acc: 0.7350\n",
      "Epoch 76/200\n",
      "0s - loss: 0.3505 - acc: 0.8437 - val_loss: 0.5064 - val_acc: 0.7400\n",
      "Epoch 77/200\n",
      "0s - loss: 0.3591 - acc: 0.8263 - val_loss: 0.5071 - val_acc: 0.7300\n",
      "Epoch 78/200\n",
      "0s - loss: 0.3489 - acc: 0.8363 - val_loss: 0.5045 - val_acc: 0.7350\n",
      "Epoch 79/200\n",
      "0s - loss: 0.3596 - acc: 0.8287 - val_loss: 0.5087 - val_acc: 0.7350\n",
      "Epoch 80/200\n",
      "0s - loss: 0.3412 - acc: 0.8500 - val_loss: 0.5155 - val_acc: 0.7350\n",
      "Epoch 81/200\n",
      "0s - loss: 0.3510 - acc: 0.8475 - val_loss: 0.5110 - val_acc: 0.7450\n",
      "Epoch 82/200\n",
      "0s - loss: 0.3478 - acc: 0.8250 - val_loss: 0.5228 - val_acc: 0.7200\n",
      "Epoch 83/200\n",
      "0s - loss: 0.3528 - acc: 0.8300 - val_loss: 0.5042 - val_acc: 0.7350\n",
      "Epoch 84/200\n",
      "0s - loss: 0.3398 - acc: 0.8475 - val_loss: 0.5223 - val_acc: 0.7250\n",
      "Epoch 85/200\n",
      "0s - loss: 0.3433 - acc: 0.8550 - val_loss: 0.5276 - val_acc: 0.7200\n",
      "Epoch 86/200\n",
      "0s - loss: 0.3443 - acc: 0.8350 - val_loss: 0.5200 - val_acc: 0.7450\n",
      "Epoch 87/200\n",
      "0s - loss: 0.3359 - acc: 0.8462 - val_loss: 0.5190 - val_acc: 0.7300\n",
      "Epoch 88/200\n",
      "0s - loss: 0.3299 - acc: 0.8513 - val_loss: 0.5105 - val_acc: 0.7450\n",
      "Epoch 89/200\n",
      "0s - loss: 0.3227 - acc: 0.8688 - val_loss: 0.5232 - val_acc: 0.7450\n",
      "Epoch 90/200\n",
      "0s - loss: 0.3440 - acc: 0.8500 - val_loss: 0.5212 - val_acc: 0.7400\n",
      "Epoch 91/200\n",
      "0s - loss: 0.3311 - acc: 0.8612 - val_loss: 0.5153 - val_acc: 0.7400\n",
      "Epoch 92/200\n",
      "0s - loss: 0.3111 - acc: 0.8800 - val_loss: 0.5287 - val_acc: 0.7150\n",
      "Epoch 93/200\n",
      "0s - loss: 0.3255 - acc: 0.8638 - val_loss: 0.5300 - val_acc: 0.7250\n",
      "Epoch 94/200\n",
      "0s - loss: 0.3221 - acc: 0.8737 - val_loss: 0.5425 - val_acc: 0.7100\n",
      "Epoch 95/200\n",
      "0s - loss: 0.3114 - acc: 0.8650 - val_loss: 0.5475 - val_acc: 0.7050\n",
      "Epoch 96/200\n",
      "0s - loss: 0.3443 - acc: 0.8350 - val_loss: 0.5221 - val_acc: 0.7300\n",
      "Epoch 97/200\n",
      "0s - loss: 0.3295 - acc: 0.8588 - val_loss: 0.5215 - val_acc: 0.7450\n",
      "Epoch 98/200\n",
      "0s - loss: 0.3108 - acc: 0.8725 - val_loss: 0.5326 - val_acc: 0.7100\n",
      "Epoch 99/200\n",
      "0s - loss: 0.3052 - acc: 0.8875 - val_loss: 0.5461 - val_acc: 0.7100\n",
      "Epoch 100/200\n",
      "0s - loss: 0.3075 - acc: 0.8613 - val_loss: 0.5582 - val_acc: 0.7050\n",
      "Epoch 101/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.2800 - acc: 0.8838 - val_loss: 0.5638 - val_acc: 0.6850\n",
      "Epoch 102/200\n",
      "0s - loss: 0.3306 - acc: 0.8500 - val_loss: 0.5684 - val_acc: 0.7050\n",
      "Epoch 103/200\n",
      "0s - loss: 0.3107 - acc: 0.8488 - val_loss: 0.5250 - val_acc: 0.7400\n",
      "Epoch 104/200\n",
      "0s - loss: 0.3015 - acc: 0.8800 - val_loss: 0.5338 - val_acc: 0.7450\n",
      "Epoch 105/200\n",
      "0s - loss: 0.2877 - acc: 0.8700 - val_loss: 0.5365 - val_acc: 0.7250\n",
      "Epoch 106/200\n",
      "0s - loss: 0.3096 - acc: 0.8800 - val_loss: 0.5381 - val_acc: 0.7150\n",
      "Epoch 107/200\n",
      "0s - loss: 0.2971 - acc: 0.8700 - val_loss: 0.5417 - val_acc: 0.7250\n",
      "Epoch 108/200\n",
      "0s - loss: 0.2941 - acc: 0.8837 - val_loss: 0.5378 - val_acc: 0.7400\n",
      "Epoch 109/200\n",
      "0s - loss: 0.3008 - acc: 0.8700 - val_loss: 0.5623 - val_acc: 0.7100\n",
      "Epoch 110/200\n",
      "0s - loss: 0.3129 - acc: 0.8537 - val_loss: 0.5613 - val_acc: 0.6950\n",
      "Epoch 111/200\n",
      "0s - loss: 0.2949 - acc: 0.8800 - val_loss: 0.6097 - val_acc: 0.6900\n",
      "Epoch 112/200\n",
      "0s - loss: 0.2981 - acc: 0.8638 - val_loss: 0.5702 - val_acc: 0.7100\n",
      "Epoch 113/200\n",
      "0s - loss: 0.2949 - acc: 0.8775 - val_loss: 0.5481 - val_acc: 0.7100\n",
      "Epoch 114/200\n",
      "0s - loss: 0.3052 - acc: 0.8687 - val_loss: 0.5569 - val_acc: 0.7450\n",
      "Epoch 115/200\n",
      "0s - loss: 0.2962 - acc: 0.8637 - val_loss: 0.5613 - val_acc: 0.7050\n",
      "Epoch 116/200\n",
      "0s - loss: 0.3043 - acc: 0.8750 - val_loss: 0.6115 - val_acc: 0.7000\n",
      "Epoch 117/200\n",
      "0s - loss: 0.2793 - acc: 0.8813 - val_loss: 0.6285 - val_acc: 0.6900\n",
      "Epoch 118/200\n",
      "0s - loss: 0.2949 - acc: 0.8775 - val_loss: 0.5866 - val_acc: 0.7150\n",
      "Epoch 119/200\n",
      "0s - loss: 0.2851 - acc: 0.8913 - val_loss: 0.6190 - val_acc: 0.7250\n",
      "Epoch 120/200\n",
      "0s - loss: 0.2793 - acc: 0.8925 - val_loss: 0.6138 - val_acc: 0.7300\n",
      "Epoch 121/200\n",
      "0s - loss: 0.2827 - acc: 0.8888 - val_loss: 0.6284 - val_acc: 0.7150\n",
      "Epoch 122/200\n",
      "0s - loss: 0.2719 - acc: 0.9062 - val_loss: 0.6748 - val_acc: 0.6900\n",
      "Epoch 123/200\n",
      "0s - loss: 0.2815 - acc: 0.8812 - val_loss: 0.6100 - val_acc: 0.6900\n",
      "Epoch 124/200\n",
      "0s - loss: 0.2639 - acc: 0.9000 - val_loss: 0.6069 - val_acc: 0.7000\n",
      "Epoch 125/200\n",
      "0s - loss: 0.2630 - acc: 0.8963 - val_loss: 0.6334 - val_acc: 0.7100\n",
      "Epoch 126/200\n",
      "0s - loss: 0.2778 - acc: 0.8850 - val_loss: 0.6542 - val_acc: 0.6950\n",
      "Epoch 127/200\n",
      "0s - loss: 0.2562 - acc: 0.9013 - val_loss: 0.6635 - val_acc: 0.7100\n",
      "Epoch 128/200\n",
      "0s - loss: 0.2716 - acc: 0.8762 - val_loss: 0.6204 - val_acc: 0.6950\n",
      "Epoch 129/200\n",
      "0s - loss: 0.2801 - acc: 0.8812 - val_loss: 0.6503 - val_acc: 0.7000\n",
      "Epoch 130/200\n",
      "0s - loss: 0.2620 - acc: 0.8912 - val_loss: 0.6770 - val_acc: 0.6950\n",
      "Epoch 131/200\n",
      "0s - loss: 0.2647 - acc: 0.8850 - val_loss: 0.5925 - val_acc: 0.7150\n",
      "Epoch 132/200\n",
      "0s - loss: 0.2637 - acc: 0.9000 - val_loss: 0.6345 - val_acc: 0.7100\n",
      "Epoch 133/200\n",
      "0s - loss: 0.2502 - acc: 0.9025 - val_loss: 0.6205 - val_acc: 0.7200\n",
      "Epoch 134/200\n",
      "0s - loss: 0.2553 - acc: 0.9088 - val_loss: 0.6181 - val_acc: 0.7050\n",
      "Epoch 135/200\n",
      "0s - loss: 0.2505 - acc: 0.9112 - val_loss: 0.6540 - val_acc: 0.7100\n",
      "Epoch 136/200\n",
      "0s - loss: 0.2446 - acc: 0.9012 - val_loss: 0.7791 - val_acc: 0.6700\n",
      "Epoch 137/200\n",
      "0s - loss: 0.2437 - acc: 0.8975 - val_loss: 0.7069 - val_acc: 0.6950\n",
      "Epoch 138/200\n",
      "0s - loss: 0.2619 - acc: 0.8938 - val_loss: 0.6417 - val_acc: 0.7100\n",
      "Epoch 139/200\n",
      "0s - loss: 0.2508 - acc: 0.9025 - val_loss: 0.6718 - val_acc: 0.6900\n",
      "Epoch 140/200\n",
      "0s - loss: 0.2567 - acc: 0.8750 - val_loss: 0.6762 - val_acc: 0.7150\n",
      "Epoch 141/200\n",
      "0s - loss: 0.2574 - acc: 0.9037 - val_loss: 0.7397 - val_acc: 0.6750\n",
      "Epoch 142/200\n",
      "0s - loss: 0.2542 - acc: 0.8975 - val_loss: 0.6765 - val_acc: 0.7000\n",
      "Epoch 143/200\n",
      "0s - loss: 0.2453 - acc: 0.9100 - val_loss: 0.6868 - val_acc: 0.7100\n",
      "Epoch 144/200\n",
      "0s - loss: 0.2542 - acc: 0.9025 - val_loss: 0.7446 - val_acc: 0.6750\n",
      "Epoch 145/200\n",
      "0s - loss: 0.2485 - acc: 0.9037 - val_loss: 0.6718 - val_acc: 0.7100\n",
      "Epoch 146/200\n",
      "0s - loss: 0.2403 - acc: 0.9100 - val_loss: 0.6745 - val_acc: 0.6900\n",
      "Epoch 147/200\n",
      "0s - loss: 0.2281 - acc: 0.9237 - val_loss: 0.7271 - val_acc: 0.6900\n",
      "Epoch 148/200\n",
      "0s - loss: 0.2340 - acc: 0.9100 - val_loss: 0.8022 - val_acc: 0.7000\n",
      "Epoch 149/200\n",
      "0s - loss: 0.2461 - acc: 0.8963 - val_loss: 0.7490 - val_acc: 0.6900\n",
      "Epoch 150/200\n",
      "0s - loss: 0.2467 - acc: 0.9037 - val_loss: 0.7410 - val_acc: 0.7050\n",
      "Epoch 151/200\n",
      "0s - loss: 0.2456 - acc: 0.9000 - val_loss: 0.6878 - val_acc: 0.6950\n",
      "Epoch 152/200\n",
      "0s - loss: 0.2262 - acc: 0.9137 - val_loss: 0.7551 - val_acc: 0.6900\n",
      "Epoch 153/200\n",
      "0s - loss: 0.2345 - acc: 0.9175 - val_loss: 0.7071 - val_acc: 0.7000\n",
      "Epoch 154/200\n",
      "0s - loss: 0.2316 - acc: 0.9062 - val_loss: 0.7785 - val_acc: 0.6850\n",
      "Epoch 155/200\n",
      "0s - loss: 0.2379 - acc: 0.9012 - val_loss: 0.7303 - val_acc: 0.7000\n",
      "Epoch 156/200\n",
      "0s - loss: 0.2150 - acc: 0.9338 - val_loss: 0.6636 - val_acc: 0.6950\n",
      "Epoch 157/200\n",
      "0s - loss: 0.2525 - acc: 0.8825 - val_loss: 0.6868 - val_acc: 0.6950\n",
      "Epoch 158/200\n",
      "0s - loss: 0.2366 - acc: 0.9100 - val_loss: 0.7315 - val_acc: 0.6900\n",
      "Epoch 159/200\n",
      "0s - loss: 0.2343 - acc: 0.8987 - val_loss: 0.7176 - val_acc: 0.7000\n",
      "Epoch 160/200\n",
      "0s - loss: 0.2338 - acc: 0.9050 - val_loss: 0.8335 - val_acc: 0.6900\n",
      "Epoch 161/200\n",
      "0s - loss: 0.2364 - acc: 0.9012 - val_loss: 0.8274 - val_acc: 0.6800\n",
      "Epoch 162/200\n",
      "0s - loss: 0.2203 - acc: 0.9187 - val_loss: 0.7422 - val_acc: 0.7000\n",
      "Epoch 163/200\n",
      "0s - loss: 0.2149 - acc: 0.9238 - val_loss: 0.7474 - val_acc: 0.7000\n",
      "Epoch 164/200\n",
      "0s - loss: 0.2374 - acc: 0.9038 - val_loss: 0.7228 - val_acc: 0.7050\n",
      "Epoch 165/200\n",
      "0s - loss: 0.2360 - acc: 0.9163 - val_loss: 0.6947 - val_acc: 0.7000\n",
      "Epoch 166/200\n",
      "0s - loss: 0.2135 - acc: 0.9187 - val_loss: 0.7791 - val_acc: 0.7050\n",
      "Epoch 167/200\n",
      "0s - loss: 0.2071 - acc: 0.9212 - val_loss: 0.6810 - val_acc: 0.7100\n",
      "Epoch 168/200\n",
      "0s - loss: 0.2207 - acc: 0.9113 - val_loss: 0.6944 - val_acc: 0.6900\n",
      "Epoch 169/200\n",
      "0s - loss: 0.2405 - acc: 0.9012 - val_loss: 0.6835 - val_acc: 0.7100\n",
      "Epoch 170/200\n",
      "0s - loss: 0.2227 - acc: 0.9163 - val_loss: 0.8455 - val_acc: 0.6950\n",
      "Epoch 171/200\n",
      "0s - loss: 0.2181 - acc: 0.9050 - val_loss: 0.7287 - val_acc: 0.7100\n",
      "Epoch 172/200\n",
      "0s - loss: 0.2175 - acc: 0.9113 - val_loss: 0.7225 - val_acc: 0.6850\n",
      "Epoch 173/200\n",
      "0s - loss: 0.2279 - acc: 0.9025 - val_loss: 0.7431 - val_acc: 0.7100\n",
      "Epoch 174/200\n",
      "0s - loss: 0.2192 - acc: 0.9175 - val_loss: 0.7759 - val_acc: 0.7000\n",
      "Epoch 175/200\n",
      "0s - loss: 0.2275 - acc: 0.9137 - val_loss: 0.7349 - val_acc: 0.7000\n",
      "Epoch 176/200\n",
      "0s - loss: 0.2123 - acc: 0.9200 - val_loss: 0.8025 - val_acc: 0.7050\n",
      "Epoch 177/200\n",
      "0s - loss: 0.2264 - acc: 0.9062 - val_loss: 0.7772 - val_acc: 0.6950\n",
      "Epoch 178/200\n",
      "0s - loss: 0.2270 - acc: 0.9050 - val_loss: 0.9174 - val_acc: 0.6850\n",
      "Epoch 179/200\n",
      "0s - loss: 0.2260 - acc: 0.9087 - val_loss: 0.9458 - val_acc: 0.6800\n",
      "Epoch 180/200\n",
      "0s - loss: 0.2389 - acc: 0.8975 - val_loss: 0.7648 - val_acc: 0.7050\n",
      "Epoch 181/200\n",
      "0s - loss: 0.2016 - acc: 0.9288 - val_loss: 0.7913 - val_acc: 0.7000\n",
      "Epoch 182/200\n",
      "0s - loss: 0.2231 - acc: 0.9175 - val_loss: 0.6997 - val_acc: 0.7200\n",
      "Epoch 183/200\n",
      "0s - loss: 0.2181 - acc: 0.9175 - val_loss: 0.7590 - val_acc: 0.6800\n",
      "Epoch 184/200\n",
      "0s - loss: 0.2191 - acc: 0.9037 - val_loss: 0.7448 - val_acc: 0.7050\n",
      "Epoch 185/200\n",
      "0s - loss: 0.2171 - acc: 0.9238 - val_loss: 0.8134 - val_acc: 0.6950\n",
      "Epoch 186/200\n",
      "0s - loss: 0.1986 - acc: 0.9225 - val_loss: 0.9383 - val_acc: 0.6800\n",
      "Epoch 187/200\n",
      "0s - loss: 0.2036 - acc: 0.9212 - val_loss: 0.9460 - val_acc: 0.6700\n",
      "Epoch 188/200\n",
      "0s - loss: 0.2101 - acc: 0.9187 - val_loss: 1.0049 - val_acc: 0.6800\n",
      "Epoch 189/200\n",
      "0s - loss: 0.2133 - acc: 0.9175 - val_loss: 0.8135 - val_acc: 0.6750\n",
      "Epoch 190/200\n",
      "0s - loss: 0.1968 - acc: 0.9338 - val_loss: 0.9319 - val_acc: 0.6850\n",
      "Epoch 191/200\n",
      "0s - loss: 0.2053 - acc: 0.9213 - val_loss: 0.8574 - val_acc: 0.6900\n",
      "Epoch 192/200\n",
      "0s - loss: 0.2017 - acc: 0.9188 - val_loss: 0.8059 - val_acc: 0.7050\n",
      "Epoch 193/200\n",
      "0s - loss: 0.2087 - acc: 0.9188 - val_loss: 0.8726 - val_acc: 0.6950\n",
      "Epoch 194/200\n",
      "0s - loss: 0.1938 - acc: 0.9313 - val_loss: 0.8411 - val_acc: 0.6950\n",
      "Epoch 195/200\n",
      "0s - loss: 0.2087 - acc: 0.9150 - val_loss: 0.8640 - val_acc: 0.6950\n",
      "Epoch 196/200\n",
      "0s - loss: 0.1938 - acc: 0.9325 - val_loss: 0.8848 - val_acc: 0.7000\n",
      "Epoch 197/200\n",
      "0s - loss: 0.2097 - acc: 0.9088 - val_loss: 0.8802 - val_acc: 0.6900\n",
      "Epoch 198/200\n",
      "0s - loss: 0.1830 - acc: 0.9400 - val_loss: 0.8157 - val_acc: 0.7050\n",
      "Epoch 199/200\n",
      "0s - loss: 0.1855 - acc: 0.9375 - val_loss: 1.0094 - val_acc: 0.7000\n",
      "Epoch 200/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.2186 - acc: 0.9162 - val_loss: 0.8918 - val_acc: 0.6900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22fb2a7fa90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the function to create the model\n",
    "model1=getModel()\n",
    "# Train the model for a fixed number of epochs\n",
    "model1.fit([data_1, data_2, data_3], Ytrain, epochs=200, batch_size=512,verbose=2,validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-ba5414a7a336>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_data_1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data_2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_data_1' is not defined"
     ]
    }
   ],
   "source": [
    "predictions=model1.predict([test_data_1,test_data_2],512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def pro of one func\n",
    "prob_of_one_udf = func.udf(lambda v: float(v[1]), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outdf = predictions.withColumn('predict', func.round(prob_of_one_udf('probability'),6)).select('id','predict')\n",
    "outdf.cache()\n",
    "outdf.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write csv\n",
    "outdf.orderBy('id').coalesce(1).write.csv(outPath,header=True,mode='overwrite',quote=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assign batch size as variable\n",
    "# lengths of vectrs we want to have\n",
    "# global variabeldimension max sequence length(30)\n",
    "# embedding layre needs embedding dimension ( 300)\n",
    "# bigger batch size, smoother gradient, accurate search, but uses memory\n",
    "\n",
    "#submission 1\n",
    "\n",
    "#is_dubplicate    test_id\n",
    "# select cutoff words"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
